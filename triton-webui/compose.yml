services:
  tensorrt-llm:
    image: triton-webui/tensorrt-llm
    build: ./docker/tensorrt-llm
    volumes:
      - data/models:/data/models:r
      - data/checkpoints:/data/checkpoints:rw
      - data/engines:/data/engines:rw
    command: python3 -c "import tensorrt_llm"
  triton:
    image: triton-webui/triton
    build: ./docker/triton
    volumes:
      - data:data:r
    command: python3 tensorrtllm_backend/scripts/launch_triton_server.py \
    --model_repo tensorrtllm_backend/all_models/inflight_batcher_llm \
    --world_size 1

  webui:
    image: ghcr.io/open-webui/open-webui:cuda
    ports:
      - "3000:8080"
    extra_hosts:
      - "host.docker.internal:host-gateway"
    volumes:
      - open-webui-local:/app/backend/data
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
              driver: nvidia
  # TODO: Split this up into two containers
  # 1. A base triton server like what athena is using
  # 2. A proxy triton openai server to bridge the backend and UI
  triton:
    image: visitsb/tritonserver:24.06-trtllm-python-py3
    command: >
      sh -c '
        /opt/tritonserver/bin/tritonserver --model-store /models/mymodel/model & 
        /opt/tritonserver/bin/tritonopenaiserver 
          --triton_http_proto=grpc
          --triton_http_port=8001
          --tokenizer_dir /models/mymodel/tokenizer 
          --engine_dir /models/mymodel/engine
      '
    ports:
      - "11434:11434/tcp"   # The OpenAI API port of our prox that open-webui needs
      - "8001:8001/tcp"     # The Triton gRPC port
    volumes:
      - <our shared models directory>:/models:r
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
              driver: nvidia
    