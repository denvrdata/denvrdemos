services:
  nim:
    # Our example image is just an 8B parameter llama 3 model
    image: nvcr.io/nim/meta/llama3-8b-instruct:1.0.0
    # To avoid permissions errors in the data/model dir
    user: 1000
    # The port for accessing the Open API endpoint
    ports:
      - 8000:8000
    # Directory for storage cached models (for faster restarts)
    volumes:
      - data/models:/opt/nim/.cache:rw
    # Set the NGC_CLI_API_KEY environment variable, so that the container can download models
    environment:
      - NGC_CLI_API_KEY: /run/secrets/ngc_api_key
    # Add the NVIDIA GPU to the container. Defaults to all GPUs
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
              driver: nvidia
    # Specify that the container needs access to the ngc_api_key secret
    secrets:
      - ngc_api_key
    # Include a health check to wait until the container is online
    healthcheck:
      test: curl --fail http://localhost:8000/v1/models
      interval: 30s
      timeout: 30s
      retries: 3
      start_period: 60s
  # webui:
  #   image: ghcr.io/open-webui/open-webui:cuda
  #   user: 1000
  #   ports:
  #     - "3000:8080"
  #   extra_hosts:
  #     - "host.docker.internal:host-gateway"
  #   volumes:
  #     - open-webui-local:/app/backend/data
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - capabilities: [gpu]
  #             driver: nvidia
# Define our secrets by point to a local file in the `secrets` directory.
secrets:
  ngc_api_key:
    file: secrets/ngc-api-key.secret
