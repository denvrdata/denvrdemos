services:
  nim:
    image: nvcr.io/nim/meta/llama3-8b-instruct:1.0.0
    shm_size: 16GB
    # Directory for storage cached models (for faster restarts)
    # We'll pull this from NFS for our example
    volumes:
      - ./data/nim:/opt/nim/.cache:rw
    # Set the NGC_CLI_API_KEY environment variable, so that the container can download models
    env_file:
      - path: ./.config/nim.env
        required: true
    # Specify our GPU
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
              driver: nvidia
              count: 1
    # Include a health check to wait until the container is online
    healthcheck:
      test: httpx http://localhost:8000/v1/models
      interval: 30s
      timeout: 30s
      retries: 10
      start_period: 180s
  webui:
    image: ghcr.io/open-webui/open-webui:v0.3.11-cuda
    env_file:
      - path: ./.config/webui.env
        required: true
    volumes:
      - ./data/webui:/app/backend/data:rw
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
              driver: nvidia
              count: 1
    healthcheck:
      test: curl --fail http://localhost:8080
      interval: 30s
      timeout: 30s
      retries: 3
      start_period: 60s
    depends_on:
      nim:
        condition: service_healthy
  caddy:
    image: caddy:latest
    ports:
      - 443:443
    volumes:
      - ./.config/caddy/Caddyfile:/etc/caddy/Caddyfile
      - ./.config/caddy:/config
      - ./data/caddy:/data
    healthcheck:
      test: curl --fail https://localhost:443
      interval: 30s
      timeout: 30s
      retries: 3
      start_period: 60s
    depends_on:
      webui:
        condition: service_healthy

